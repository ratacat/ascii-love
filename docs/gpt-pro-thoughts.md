System Architecture and Export Model Analysis for ASCII Asset Editor
1. Recommended Data Model Structure (.love.json or Alternative)
For a browser-based ASCII art editor, a structured JSON scene graph (provisionally called .love.json) is the optimal data model. This format serves as the canonical source of truth for assets, capturing all essential metadata: layers, glyph instances (characters), positions, Z-order, palette references, and groups. In practice, each glyph would be stored with its character code (or Unicode code point), coordinates on the canvas grid, color information (often referencing a palette index or swatch name), and the layer it belongs to. The JSON can also include transformation data per glyph (e.g. scale or rotation) and tagging or grouping metadata for interactivity (e.g. grouping several glyphs into a named entity like a “skill node”). By designing the data model as a rich scene description, we ensure fidelity is preserved across saves/loads – all layering, theming, and interactive hooks remain intact, unlike in flat image formats. This approach is similar to how professional ASCII editors like REXPaint handle data: REXPaint’s native format stores a stack of layers with each cell’s char and color values, which is then exported to game-ready formats as needed
discourse.cataclysmdda.org
. A custom JSON schema offers human-readability and versionability (we can include a schemaVersion field for future-proofing), making it easy to maintain. Alternative data models were considered – for example, a plain text matrix per layer or repurposing a tilemap format – but those lack flexibility for arbitrary positioning, grouping, and per-glyph metadata. The .love.json structure, in contrast, can capture non-grid transformations (rotated or scaled characters), cross-layer groupings, and palette tokens. It essentially functions as a mini scene graph tailored to ASCII content. The editor can use this structure in-memory during editing, and save it as the “source file” for assets. This clean separation of editable source vs. runtime export aligns with best practices (similar to how a Photoshop .psd is separate from a exported .png). In summary, a custom JSON data model is recommended for maximum flexibility and ease of maintenance, ensuring that assets can be re-opened and tweaked without loss of detail.
2. Export Format Strategy (SVG, PNG, WebGL-Ready)
With the .love.json as the master representation, the editor can provide multiple export pathways for use in the game. Each export format has trade-offs in fidelity, performance, and ease of integration:
SVG (Structured Vector Graphics): Exporting an asset to SVG preserves the vector-like fidelity of the ASCII art. Each glyph can be represented as a text element or shape in the SVG, maintaining crisp edges at any scale. This format aligns well with existing web pipelines – in fact, the project’s UI borders are already SVG-based. An SVG export would keep characters sharp on high-DPI or varying screen sizes, and can support layering by grouping elements or ordering them in the SVG DOM. It also allows per-group styling: for example, different groups of glyphs could be wrapped in <g> containers with distinct CSS classes for fill color, enabling some dynamic recoloring or styling via CSS/JS if the SVG is embedded inline. The downside is performance and complexity if used directly in the game: an SVG with thousands of <text> elements can bloat the DOM and slow down rendering or manipulation. Careful ordering is needed to encode the z-index of overlapping glyphs (so that higher-layer glyphs appear on top). Animations in pure SVG are limited – one could use CSS animations or SVG SMIL for simple effects, but complex interactive animations would require script control of SVG elements, which can become unwieldy. In short, SVG exports are excellent for static or lightly-interactive assets where scalability is important and existing tooling expects an image format. They provide a quick integration path since the game can simply load the SVG as an image or embed it directly.
Raster Images (PNG/WebP + Metadata): For final assets that don’t require dynamic behavior, exporting a raster image is the simplest route. A PNG (lossless) or WebP (which can be lossless or higher compression) gives you a ready-to-use sprite. This is effectively “baking” the ASCII art into pixels – the browser or game engine can render it extremely fast (a single textured quad is faster than drawing many glyphs individually; indeed, nothing beats the performance of a flat image in rendering speed). As one observer wryly noted, “You know what’s even faster than Canvas? JPEG.” – the point being that a pre-rendered image has virtually no runtime overhead beyond a blit
news.ycombinator.com
. The advantage of PNG/WebP exports is in static backgrounds or large scenery pieces where no per-glyph interaction is needed. They also avoid any font rendering discrepancies (the look is exactly as designed). PNG is recommended if exact pixel fidelity is needed (it supports indexed palettes which could keep file size down if the art uses a limited color palette), whereas WebP might yield smaller files for complex images at the cost of potential compression artifacts if used in lossy mode. The major trade-off is loss of dynamic flexibility: once baked, you cannot recolor or animate individual glyphs at runtime without re-rendering or using image filters. To mitigate that, one could export metadata alongside the image – e.g. a JSON mapping regions or group IDs to coordinates – so the game knows, for instance, where clickable areas or group positions are. This is useful for hit-testing or tooltips, but it doesn’t solve visual changes. Thus, raster exports are best for content that will remain visually static. They could serve as an optimization fallback (for example, if certain complex scenes prove too slow to render dynamically on low-end devices, you could choose to ship a pre-rendered image). Overall, raster exports trade away interactivity for maximum runtime performance.
Game-Native JSON (.love.json Scene): Another strategy is to use the .love.json itself (or a streamlined variant of it) as an export that the game client loads and renders on the fly. In this case, the JSON is not just an editor format but a runtime asset format describing the scene graph of glyphs. The benefit is full fidelity and flexibility: the game’s rendering system can interpret this data to draw the ASCII art, and because it has all the structured info, it can enable runtime animations (moving or fading glyphs, swapping colors via palette), interactive highlighting, responsive scaling, etc. This format is extremely developer-friendly and future-proof – it’s human-readable and easily extensible. In terms of integration, using a JSON scene means the game must have a rendering mechanism (Canvas or WebGL code) to draw the asset; unlike SVG or PNG, you can’t just drop a JSON into an <img> tag. However, this approach decouples the asset’s data from its rendering, allowing the game to apply global effects or theming. For example, if a color in the palette is labeled "accent":"theme:skill-accent" (a token) in the JSON, the game can resolve it to the current theme’s actual color and even change it at runtime to recolor the asset. The trade-off here is that the onus of rendering shifts to the game engine, which is more complex than using static assets. Initially, the JSON approach can be used side-by-side with simpler outputs: the editor saves .love.json as the master file, but you might not consume it directly in the game until the engine is ready. In practice, maintaining the JSON as the source-of-truth and having other exports derive from it is a sensible hybrid strategy. We recommend designing the JSON as the primary export for long-term flexibility, even if in the short term it’s accompanied by static exports for convenience.
WebGL-Ready Draw Data: Looking further ahead, if performance requirements grow (for instance, many animated ASCII elements on screen at once), a specialized export for WebGL could be introduced. This would involve converting the scene data into a form directly usable by a GPU renderer – e.g. a set of vertex buffers containing quad positions, UV coordinates for a font texture atlas, color indices, etc.. Essentially, this is an optimized binary or structured dataset derived from the JSON, meant to be fed into an instanced draw call. The design doc suggests deferring this until the WebGL path is clearer, which is wise; it’s an advanced optimization that isn’t needed for MVP. When/if implemented, this export might output a binary blob or a JavaScript module that initializes GPU buffers representing the asset. The advantage would be eliminating parsing or setup cost at runtime – the heavy lifting (layout to vertex conversion) would be done offline. However, maintaining this in parallel with the JSON could add complexity, so it should be added only when profiling shows a bottleneck. In many cases, the game can perform a one-time conversion from .love.json to GPU buffers at load time and achieve the same result without a separate file. So, this is a future enhancement for high-performance mode, ensuring that extremely large scenes or high frame-rate animations remain smooth. In summary, it provides a path to scale up rendering performance via instanced glyph rendering, but we recommend postponing this until phase 2+ of the project.
Given all the above, the recommended export strategy is a hybrid approach: use the .love.json as the master format and build modular exporters for other needs. Initially, focus on SVG export (to integrate with existing workflows) and possibly a basic PNG export for quick previews or static uses. As the engine matures, move toward consuming the JSON directly for interactive/animated elements. By designing the editor with a pluggable export system, new formats (like the WebGL draw-data or others) can be added later without reworking the core. This balances immediate needs (simplicity and compatibility) with future needs (performance and flexibility).
3. Runtime Rendering Options and Tradeoffs
The choice of export format directly influences how assets are rendered in the game at runtime. There are three broad options for the game’s rendering path: use pre-rendered graphics (images or SVG), use a Canvas 2D renderer, or use a WebGL renderer. Each has trade-offs in complexity and capability:
Using Pre-rendered Assets (PNG/SVG): The simplest integration is to treat exported ASCII art images just like any other sprite or image in the game. For example, a character portrait exported as portrait.png can be shown with an <img> tag or as a background texture in a canvas/WebGL engine with virtually no custom code. This has minimal CPU overhead – the browser’s rendering pipeline is highly optimized for blitting images. It’s also very low-effort: no need to re-draw glyphs at runtime. However, this path sacrifices any dynamic behavior. If you need to change a character’s colors or animate part of the portrait (say make the eyes blink), you’d have to swap out the entire image or overlay another image. There’s no granular control. In static contexts, this is fine (and very performant) – for instance, a non-interactive landscape background could just be a PNG. But for interactive UI elements or anything that should respond to game state, a flat image is limiting. An SVG used as an <img> is effectively the same as a flat image (though it scales better). An alternative is to embed the SVG inline (so its DOM is accessible). This does allow some dynamic control: you could find an element in the SVG by ID and change its color or show/hide it via JavaScript. For example, a skill tree exported to SVG could have each node as a <circle> or <text> element with an ID, and the game could highlight a node by changing its CSS class. This provides some interactivity without a custom renderer, but it’s clunky for large-scale animation – updating many SVG DOM nodes can become slow, and coordinating complex animations (like moving many glyphs) through the DOM API is cumbersome. In summary, pre-rendered assets (especially raster) are best for static usage, while SVG with DOM access offers limited dynamic capabilities with ease of integration.
Canvas 2D Rendering (Live Rasterization): Incorporating a Canvas-based renderer means the game will, at runtime, iterate over the .love.json scene data and draw each glyph to an HTML5 canvas. This can be done with standard Canvas API calls – e.g., setting a monospaced font and using fillText() for each character, or drawing from a spritesheet of bitmap font glyphs. The trade-off here is development effort vs. dynamic power. Writing a custom renderer involves managing the drawing order (to respect layers), handling positions and possibly transformations, and implementing color (which might involve setting fillStyle for each glyph or using tinted sprite bitmaps). The payoff is complete control: the game can recolor portions of the asset by simply drawing them with a different color, animate by re-drawing at intervals, or respond to input (e.g., highlight a glyph on hover by drawing a different background under it). For moderate-sized assets, Canvas rendering is quite feasible – modern browsers can draw on the order of thousands of filled text glyphs at 60 FPS. Empirical tests have shown Canvas can handle up to around 10,000 simple objects at interactive framerates (beyond which it may start to drop below 60Hz)
imld.de
. That is usually plenty for ASCII art scenarios (a typical ASCII portrait or UI might be a few thousand characters at most). Performance can be optimized by drawing static layers to an off-screen canvas once and then compositing them with dynamic layers each frame, to avoid re-drawing everything when only one part animates. The Canvas approach is an excellent phase-1 rendering solution for dynamic content: it’s simpler to implement than a full WebGL engine and leverages the browser’s well-tested 2D rendering. The main limitations are resolution-dependent rendering (a canvas is pixel-based, so scaling it up could blur the art if not handled by re-drawing at a higher resolution) and potential performance issues on very large scenes or low-power devices. The former can be addressed by drawing at double resolution for high-DPI screens or by adjusting font size – since we have vector data, we can always redraw at a needed size (unlike a fixed image)
sitepoint.com
. The latter – performance – only becomes a concern if we push the number of glyphs or update frequency into the extreme; in such cases, profiling and partial optimizations (or considering WebGL) would be next steps.
WebGL Rendering (GPU-Accelerated): A WebGL-based approach takes the rendering workload off the CPU and leverages the GPU for potentially massive numbers of glyphs or effects. This is the most complex to implement but offers the highest ceiling for performance and visual effects. In a WebGL renderer, one would typically treat each character as a textured quad (using a texture atlas of the font glyphs, or possibly a signed-distance-field font for scalability). Using instanced drawing, thousands of glyphs can be rendered in a single GPU call, with each instance using attributes for position, glyph ID (atlas UV coords), color, etc.
webglfundamentals.org
webgl.brown37.net
. The palette-based coloring fits well here: you can send a palette lookup table to the GPU and just send palette indices per glyph, allowing instant recoloring by updating one uniform array. The payoff is that even very large scenes (tens of thousands of glyphs) or frequent animations (every frame updates) can remain smooth, as WebGL can handle far more objects without dropping frames
imld.de
imld.de
. It also opens the door to special effects – for example, one could write a fragment shader to simulate a CRT scanline effect on the ASCII art, or to gradually fade out glyphs with a shader animation, etc. The downside, of course, is development complexity and overhead. One might use a library like PixiJS (which has a BitmapText feature) or Three.js sprites to avoid writing all the low-level WebGL code; these libraries batch draw calls internally to achieve similar performance with less boilerplate. Indeed, a study comparing SVG, Canvas, and WebGL for lots of text found that WebGL (implemented via Pixi in that case) stayed performant even as SVG/Canvas began to chug, especially once text nodes climbed into the many thousands
imld.de
imld.de
. For our project, a WebGL path is likely overkill in the initial phase – unless we discover early that Canvas cannot meet our needs, the wiser course is to postpone this until we truly need that power. We can architect the system to be ready for a WebGL plugin later (for example, designing the .love.json format to include everything a WebGL renderer would need, and structuring the rendering code so that swapping the backend is feasible). In practice, this might mean writing an abstraction around drawing glyphs so that, say, in phase 1 it uses Canvas calls, and in phase 2 we introduce a WebGL code path. The user experience should remain the same, just faster or with more effects. WebGL would become more important if we implement continuous animations, large animated scenes, or cross-fade effects that push Canvas to its limits, or if we target platforms where GPU acceleration is crucial.
In summary, using static SVG/PNG assets is simplest but static, Canvas rendering offers a good balance of flexibility and simplicity for moderate interactivity, and WebGL provides maximum performance and dynamic capabilities at the cost of complexity. For the text-based game’s needs, we anticipate starting with Canvas (or even SVG for some UI components) and only moving to WebGL if profiling shows a need. This phased approach aligns with the project goals of keeping the editor and engine maintainable. Crucially, by basing the runtime rendering on the structured data (the scene graph), we ensure that the engine can target specific glyph groups for effects – for example, the game could find all glyphs tagged as "highlight" in the JSON and flash them, or recolor all "enemy" tagged characters to red to indicate a status effect. These kinds of hooks are impossible with a flat image and would be awkward even with SVG, but straightforward when the game logic can manipulate the rendered output of a JSON-driven renderer. Thus, for runtime flexibility, a live renderer (Canvas or WebGL) is ultimately the goal, even if we fall back to pre-rendered graphics in some cases for expedience.
4. Implications for Editor and Toolchain Design
Choosing a rich data model and multiple export formats has direct implications on how we design the editor and the overall asset pipeline. First, adopting .love.json as the master structure means the editor must maintain a full scene graph internally. This is a logical approach: the editor’s state can mirror the JSON schema (layers, glyph instances, groups, etc.), making save/export a straightforward serialization. It also simplifies loading: the editor can import a .love.json file and reconstruct the canvas exactly, allowing iterative edits. We should include a schemaVersion in the JSON so that if we evolve the format (say, to add an animation section in v2), the editor can handle older files gracefully (e.g., via migrations or backward-compatibility code). Because we plan to support multiple export targets, the editor should be built with a modular exporter architecture. In practice, this means writing the core drawing/scene logic once, and then having separate modules/functions that convert the internal scene data into the desired output format. The design doc explicitly calls for a “shared registry” of exporters. For example, one exporter takes the scene data and produces an SVG string (using perhaps an SVG template, looping through each layer and glyph to output XML), while another exporter rasterizes the layers to an off-screen canvas and saves a PNG. A future WebGL exporter might take the scene and generate a binary buffer. By isolating these, we ensure adding a new format doesn’t destabilize the editor – it’s an additive plugin. During MVP, we’d implement the core ones (SVG and the game’s JSON) and stub out or design the interfaces for the rest. The file persistence strategy should clearly separate the editable source from exports: when a user saves their work, it produces a .love.json (which retains all info like guides, grouping tags, etc.), and when they explicitly export, then an SVG/PNG/etc is generated. This avoids confusion between “working files” and “game-ready assets.” It also maps well to version control: the .love.json (being JSON text) can be diffed and managed in Git, enabling collaborative workflows and easy tracking of changes. The need for rich layering, Z-order, and palettes means the editor’s UI must support those concepts. We will have a layers panel (to reorder, name, toggle visibility/locking of layers) that corresponds directly to the layers array in the JSON. The notion of palette-based colors suggests a palette manager in the editor: the designer can define named colors or theme references, and assign them to glyphs. The JSON will store references like { paletteId: "main", swatchId: "accent" } instead of raw color codes. The editor must ensure these references are kept consistent and allow the user to change a palette color globally. This is important for maintainability – for instance, if you decide the “accent” color should be a different shade of blue, you update the palette once rather than every glyph. The toolchain then needs to propagate these palette choices to the game. In phase 1, since we might export to SVG, one approach is embedding the palette colors directly (resolving them to hex in the SVG). But for future dynamic theming, we might instead export theme tokens (as custom data attributes or class names in SVG), which the game can post-process. This level of coordination requires clear documentation of how palette tokens in .love.json map to game theme variables. It’s a design choice: we could have the editor optionally bake in concrete colors for a quick export, and still keep the abstract token in the JSON for later use. The editor’s responsibility is to manage these references cleanly so that fidelity is preserved (e.g., avoid “baking in” a color in the JSON when it’s meant to stay theme-referential). Interactivity and animation, even if not fully built in v1, influence the design. The editor will allow grouping of glyphs (e.g., selecting a set of glyphs and marking them as a named group) for organizational purposes and for runtime hooks. Although we won’t implement an animation timeline initially, we anticipate that a “group” might later correspond to an animation target (like “pulse this group’s color”). So the editor should let the user tag groups with metadata flags (the design doc calls them runtimeFlags like themeTarget or animTarget). These flags and tags travel in the JSON to inform the game which groups are intended to be dynamic. The implication for the editor is minimal (just UI to toggle those flags), but it’s crucial for the pipeline. Moreover, because we foresee possibly integrating a WebGL preview or advanced rendering later, the editor’s core should remain renderer-agnostic. Right now, the editor can simply use the Canvas 2D API to draw the ASCII art on screen for the user (since that’s easy to set up in a React app and fast enough for editing). But if, say, in phase 4 we want to preview WebGL effects in-editor, we might embed a WebGL context. By keeping the drawing logic somewhat abstract (e.g., having a rendering module that can have multiple backends), we ease that future integration. That said, YAGNI principle applies – we won’t over-engineer the editor for a WebGL preview now, just ensure it’s not impossible later (e.g., avoid tying the editor UI too tightly to Canvas-specific features). The design principle stated was to “focus on 2D ASCII composition first” and not build a full WYSIWYG WebGL editor upfront, which we will follow. From a toolchain perspective, we should also consider how assets move from the editor to the game. Since the editor is standalone (likely running locally or as a separate webapp), exported files need to be passed to the game’s build or loaded at runtime. We could support exporting directly into the game’s assets folder (if run in a developer environment) or simply have the user drop the exported files into the game project. If the game is in React, it might import an SVG or JSON. For example, a .love.json could be fetched via an HTTP request or imported as a module (if bundled). Ensuring the format is simple JSON means no extra parsing library is needed – just fetch() and JSON.parse in the game. The team might also build a small loader utility on the game side (e.g., a function loadLoveAsset(json) that constructs the necessary draw calls or objects). Because the editor and runtime may share logic, we might factor out a small shared library – for instance, a JavaScript module that knows how to draw a .love.json on a given Canvas. This could be used in the editor (for the canvas preview) and in the game (for the actual rendering) to avoid code duplication. As long as the game doesn’t depend on any editor-specific code (just this shared rendering logic), it fits the requirement that exported assets not depend on editor runtime code. Essentially, we can extract a rendering engine module that both environments use. This improves maintainability: any change in how we render glyphs (say, kerning adjustments or how rotation is handled) can be updated in one place. Finally, we need to ensure that the editorial workflow remains manageable as features grow. Adding more export formats or integrating with the runtime should not require redesigning the editor’s core. That’s why we prioritize a clean separation of concerns: the editor manages content creation (layout, layers, groups, palettes) and produces a rich data representation; the exporters/translators handle output; the game handles drawing. By keeping these boundaries clear, each part remains understandable and testable. For example, if a bug arises where a certain glyph isn’t colored correctly in-game, we can inspect whether the JSON from the editor had the right palette reference, and whether the game’s renderer interpreted that correctly – a clear contract makes it easier to pinpoint the issue. This modular approach also makes the system future-flexible. If tomorrow we wanted to support a new output (say, an ANSI .txt art export for fun, or an animated GIF export of an asset), we could do so without altering the fundamental data model – just add a new exporter plugin. Likewise, if the game engine technology shifts (imagine moving from Canvas to WebGL or even to a native app), as long as we can read the JSON, we can write a new renderer for it, and all existing assets remain usable. To summarize, the editor and toolchain are being built with maintainability and extensibility in mind: a robust intermediate format, plugin-like exporters, shared rendering code for consistency, and clear delineation between the creation phase and the consumption phase. This ensures the system can grow (supporting new features like theming, animations, or new platforms) without extensive rework, fulfilling the project’s goal of a future-flexible asset pipeline.
5. Summary Recommendations: Phase 1 vs Phase 2 Implementation
Taking into account the above analysis, we can outline a phased implementation strategy: Phase 1 (MVP and Initial Integration): Start by implementing the ASCII editor with the core features and basic export capabilities. In this phase, the priority is to get the tool producing assets that the game can immediately use, while establishing a strong foundation for future expansion. Concretely, this means the editor will fully support the .love.json data model – users can create layered ASCII art with palette colors and group tags, and save their work to .love.json. Alongside that, implement an SVG export path (and/or PNG export) so that each asset can be turned into something the game accepts right now. The game, in phase 1, can treat these exports as static assets (for example, use the SVGs for UI overlays and portraits). This approach lets the team integrate new artwork quickly without waiting for engine changes. During this phase, do not require the game runtime to parse .love.json – we will rely on the static exports. The editor should also be kept simple in terms of rendering tech (use Canvas 2D for the on-screen drawing in the editor for now, since it’s easier to develop and sufficient for an editing interface). Essentially, Phase 1 delivers the minimum playable content: the designer can make an asset in the editor, export to SVG/PNG, and drop it into the game. It achieves immediate art pipeline functionality while validating the .love.json schema design. According to the planning document, this corresponds to the MVP (save/load .love.json, SVG export, etc.) plus perhaps some polish on the editor UI. We should also use this phase to get feedback: ensure the data model covers the needs (are we capturing everything the artist wants to do? Are the exports meeting the game’s needs?). Any missing pieces in the schema can be addressed now, while the system is still small. Phase 2 (Extended Runtime Features and Optimization): In the second major phase, focus on runtime integration and advanced rendering capabilities. With the editor stable and artists producing assets, the next step is to make the game client “understand” the .love.json format and harness it for dynamic rendering. Practically, this means developing a loader and renderer in the game: e.g., a React component or game subsystem that can take a .love.json (probably embedded or fetched as JSON) and draw it using Canvas or WebGL. Initially, a Canvas implementation can be written to interpret the scene graph and render it on an HTML5 canvas within React. This will unlock features like palette swapping and per-glyph animations in-game – for example, the game could load a skill tree description and on a level-up event, highlight a group of glyphs by changing their color or animating them, all by manipulating the data in memory and re-drawing. During this phase, we’ll likely implement the palette theming hooks: the runtime can read color tokens from the JSON and map them to the game’s current theme values (dark mode, faction colors, etc.), allowing dynamic recoloring of ASCII art to match context. We also introduce any optional raster optimization: if there are known static heavy scenes (say a complex landscape background), we could add a PNG export for those and have the game use the PNG for that one case, while using dynamic rendering for the interactive parts. Essentially the game can have a hybrid: some assets via images, some via live rendering, depending on needs. As part of Phase 2, it’s prudent to address performance – profile the Canvas renderer with real game scenarios. If we find that we’re hitting performance limits (e.g., an ASCII animation with thousands of glyphs is sluggish), we then consider moving to the WebGL instanced rendering path. This might be a sub-step within Phase 2 or a Phase 3 in the doc’s terms, but the key is: don’t premature-optimize. Only build the WebGL pipeline once we confirm it’s needed. When we do, we’ll leverage the groundwork laid earlier (the structured data and perhaps an updated exporter). We might create a WebGL draw-data exporter or runtime converter that takes the .love.json and initializes GPU buffers for even faster drawing. This will be especially useful for fluid animations like blinking text, rain overlays made of ASCII characters, or other effects that involve many redraws. Phase 2 is also when we can layer in more interactive polish. For example, now that the game knows about groups (from JSON), we can implement clicking or hovering logic: e.g., hover over a glyph group and show a tooltip, or click to trigger an event, since the engine can map screen coordinates back to the ASCII grid and group identifiers. In parallel, the editor might also get enhancements in this phase to support the new runtime features – for instance, an animation preview mode or the ability to define simple animations (like “this group toggles color”). We might not fully simulate the game’s animation in editor, but at least allow defining the intent (as metadata). Throughout this phase, maintain backward compatibility: assets created in Phase 1 (which are in .love.json v1) should continue to work. Our loader should read the old schemaVersion and adapt if needed (though ideally, we keep the same structure). The modular exporter design from Phase 1 pays off here, as we can add a new exporter (say, WebGLBufferExporter) without breaking the others. In summary, Phase 1 delivers a working editor and static export pipeline (JSON + SVG/PNG) to get content into the game quickly. Phase 2 builds on that by integrating the JSON format into the game’s rendering engine, enabling dynamic ASCII graphics and richer visual effects. This phased approach ensures that early on, the team can produce art and see it in-game (critical for an iterative design process), while not losing sight of the longer-term vision of interactive ASCII rendering. It balances immediate practicality with future-proofing. By Phase 2’s end, we expect to have a maintainable, flexible system: the editor continues to serve as the source of truth for asset creation, and the game can either consume the editor’s exports directly or even invoke the editor’s logic (via shared libraries) to render on the fly, as needed. The result is a pipeline that can deliver static images when we just need something quick, but also empower the game to treat ASCII art as a living, themable part of the experience when desired. This strategy of “start simple, then augment” follows the project’s design principles, de-risking the development by getting basics working, and then layering on complexity in a controlled, well-architected manner.